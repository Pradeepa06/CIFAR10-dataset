{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traincollab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nprPaE31X-78"
      },
      "source": [
        "#Training the dataset\r\n",
        "\r\n",
        "#IMPORTING\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from time import time\r\n",
        "import math\r\n",
        "\r\n",
        "from include.data import get_data_set\r\n",
        "from include.model import model, lr\r\n",
        "\r\n",
        "#DECLARATION\r\n",
        "\r\n",
        "train_x, train_y = get_data_set(\"train\")\r\n",
        "test_x, test_y = get_data_set(\"test\")\r\n",
        "tf.set_random_seed(21)\r\n",
        "x, y, output, y_pred_cls, global_step, learning_rate = model()\r\n",
        "global_accuracy = 0\r\n",
        "epoch_start = 0\r\n",
        "\r\n",
        "\r\n",
        "# PARAMS\r\n",
        "_BATCH_SIZE = 128\r\n",
        "_EPOCH = 60\r\n",
        "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\r\n",
        " \r\n",
        " \r\n",
        "# LOSS AND OPTIMIZER\r\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\r\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(loss, global_step=global_step)\r\n",
        "                                                             \r\n",
        "                               \r\n",
        "# PREDICTION AND ACCURACY CALCULATION\r\n",
        "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))\r\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n",
        " \r\n",
        "# SAVER\r\n",
        "merged = tf.summary.merge_all()\r\n",
        "saver = tf.train.Saver()\r\n",
        "sess = tf.Session()\r\n",
        "train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\r\n",
        " \r\n",
        " \r\n",
        "try:\r\n",
        "    print(\"Restoring the latest checkpoint ...\")\r\n",
        "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\r\n",
        "    saver.restore(sess, save_path=last_chk_path)\r\n",
        "    print(\"Checkpoint Restored:\", last_chk_path)\r\n",
        "except ValueError:\r\n",
        "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\r\n",
        "    sess.run(tf.global_variables_initializer())\r\n",
        " \r\n",
        " \r\n",
        "def train(epoch):\r\n",
        "    global epoch_start\r\n",
        "    epoch_start = time()\r\n",
        "    batch_size = int(math.ceil(len(train_x) / _BATCH_SIZE))\r\n",
        "    i_global = 0\r\n",
        " \r\n",
        "    for s in range(batch_size):\r\n",
        "        batch_xs = train_x[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\r\n",
        "        batch_ys = train_y[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\r\n",
        " \r\n",
        "        start_time = time()\r\n",
        "        i_global, _, batch_loss, batch_acc = sess.run(\r\n",
        "            [global_step, optimizer, loss, accuracy],\r\n",
        "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)})\r\n",
        "        duration = time() - start_time\r\n",
        " \r\n",
        "        if s % 10 == 0:\r\n",
        "            percentage = int(round((s/batch_size)*100))\r\n",
        " \r\n",
        "            bar_len = 29\r\n",
        "            filled_len = int((bar_len*int(percentage))/100)\r\n",
        "            bar = '=' * filled_len + '>' + '-' * (bar_len - filled_len)\r\n",
        " \r\n",
        "            msg = \"Global step: {:>5} - [{}] {:>3}% - acc: {:.4f} - loss: {:.4f} - {:.1f} sample/sec\"\r\n",
        "            print(msg.format(i_global, bar, percentage, batch_acc, batch_loss, _BATCH_SIZE / duration))\r\n",
        " \r\n",
        "    test_and_save(i_global, epoch)\r\n",
        " \r\n",
        " \r\n",
        " def test_and_save(_global_step, epoch):\r\n",
        "    global global_accuracy\r\n",
        "    global epoch_start\r\n",
        " \r\n",
        "    i = 0\r\n",
        "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\r\n",
        "    while i < len(test_x):\r\n",
        "    j = min(i + _BATCH_SIZE, len(test_x)) \r\n",
        "    batch_xs = test_x[i:j, :]\r\n",
        "    batch_ys = test_y[i:j, :] \r\n",
        "    predicted_class[i:j] = sess.run( y_pred_cls, feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)} ) \r\n",
        "    i = j \r\n",
        "    correct = (np.argmax(test_y, axis=1) == predicted_class)\r\n",
        "    acc = correct.mean()*100 \r\n",
        "    correct_numbers = correct.sum()\r\n",
        "    hours, rem = divmod(time() - epoch_start, 3600) minutes,\r\n",
        "    seconds = divmod(rem, 60)\r\n",
        "    mes = \" Epoch {} - accuracy: {:.2f}% ({}/{}) - time: {:0>2}:{:0>2}:{:05.2f}\"\r\n",
        "    print(mes.format((epoch+1), acc, correct_numbers, len(test_x), int(hours), int(minutes), seconds))\r\n",
        " \r\n",
        "    if global_accuracy != 0 and global_accuracy < acc: \r\n",
        "        summary = tf.Summary(value=[ tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc), ])\r\n",
        "        train_writer.add_summary(summary, _global_step) \r\n",
        "        saver.save(sess, save_path=_SAVE_PATH, global_step=_global_step) \r\n",
        "        mes = \"This epoch receive better accuracy: {:.2f} > {:.2f}. Saving session...\"\r\n",
        "        print(mes.format(acc, global_accuracy))\r\n",
        "        global_accuracy = acc\r\n",
        " \r\n",
        "    elif global_accuracy == 0:\r\n",
        "        global_accuracy = acc\r\n",
        " \r\n",
        "    print(\"-------------------------------------------\")\r\n",
        " \r\n",
        " \r\n",
        "def main():\r\n",
        "    train_start = time()\r\n",
        " \r\n",
        "    for i in range(_EPOCH):\r\n",
        "        print(\"Epoch: {}/{}\".format((i+1), _EPOCH))\r\n",
        "        train(i)\r\n",
        " \r\n",
        "    hours, rem = divmod(time() - train_start, 3600)\r\n",
        "    minutes, seconds = divmod(rem, 60)\r\n",
        "    mes = \"Best accuracy pre session: {:.2f}, time: {:0>2}:{:0>2}:{:05.2f}\"\r\n",
        "    print(mes.format(global_accuracy, int(hours), int(minutes), seconds))\r\n",
        " \r\n",
        " \r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()\r\n",
        " \r\n",
        " \r\n",
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_29-7VTpYKsJ"
      },
      "source": [
        "Epoch: 60/60\r\n",
        "\r\n",
        "\r\n",
        "Global step: 23070 - [>--------------]   0% - acc: 0.9531 - \r\n",
        "loss: 1.5081 - 7045.4 sample/sec\r\n",
        "\r\n",
        "Global step: 23080 - [>-----------]   2% - acc: 0.9453 - loss: 1.5159 - 7147.6 sample/sec\r\n",
        "\r\n",
        "Global step: 23090 - [=>---------------]   7% - acc: 0.9844 - loss: 1.4764 - 7152.6 sample/sec\r\n",
        "\r\n",
        "Global step: 23100 - [==>----------------]   9% - acc: 0.9297 - loss: 1.5307 - 7104.4 sample/sec\r\n",
        "\r\n",
        "Global step: 23110 - [==>-------------]  17% - acc: 0.9141 - loss: 1.5462 - 7091.4 sample/sec\r\n",
        "\r\n",
        "Global step: 23120 - [===>-------------]  15% - acc: 0.9297 - loss: 1.5314 - 7152.9 sample/sec\r\n",
        "\r\n",
        "Global step: 23130 - [====>-------------]  20% - acc: 0.9297 - loss: 1.5307 - 7134.8 sample/sec\r\n",
        "\r\n",
        "Global step: 23140 - [=====>--------------]  25% - acc: 0.9375 - loss: 1.5231 - 7140.0 sample/sec\r\n",
        "\r\n",
        "Global step: 23150 - [=====>--------------]  30% - acc: 0.9297 - loss: 1.5301 - 7162.8 sample/sec\r\n",
        "\r\n",
        "Global step: 23160 - [======>-----------]  32% - acc: 0.9531 - loss: 1.5080 - 7112.3 sample/sec\r\n",
        "\r\n",
        "Global step: 23170 - [=======>----------]  37% - acc: 0.9609 - loss: 1.5000 - 7154.0 sample/sec\r\n",
        "\r\n",
        "Global step: 23180 - [========>------------]  40% - acc: 0.9531 - loss: 1.5074 - 6862.2 sample/sec\r\n",
        "\r\n",
        "Global step: 23190 - [========>------------]  42% - acc: 0.9609 - loss: 1.4993 - 7134.5 sample/sec\r\n",
        "\r\n",
        "Global step: 23200 - [=========>----------]  45% - acc: 0.9609 - loss: 1.4995 - 7466.0 sample/sec\r\n",
        "\r\n",
        "Global step: 23210 - [==========>-----------]  48% - acc: 0.9375 - loss: 1.5231 - 7116.7 sample/sec\r\n",
        "\r\n",
        "Global step: 23220 - [===========>------------]  50% - acc: 0.9453 - loss: 1.5153 - 7124.1 sample/sec\r\n",
        "\r\n",
        "Global step: 23230 - [===========>-----------]  52% - acc: 0.9375 - loss: 1.5233 - 7094.5 sample/sec\r\n",
        "\r\n",
        "Global step: 23240 - [============>-----------]  53% - acc: 0.9219 - loss: 1.5387 - 7173.9 sample/sec\r\n",
        "\r\n",
        "Global step: 23250 - [=============>---------]  56% - acc: 0.8828 - loss: 1.5769 - 7184.1 sample/sec\r\n",
        "\r\n",
        "Global step: 23260 - [==============>--------]  59% - acc: 0.9219 - loss: 1.5383 - 7079.7 sample/sec\r\n",
        "\r\n",
        "Global step: 23270 - [==============>----------]  61% - acc: 0.8984 - loss: 1.5618 - 6628.6 sample/sec\r\n",
        "\r\n",
        "Global step: 23280 - [===============>---------]  64% - acc: 0.9453 - loss: 1.5151 - 7045.7 sample/sec\r\n",
        "\r\n",
        "Global step: 23290 - [================>-------]  66% - acc: 0.9609 - loss: 1.4996 - 7189.0 sample/sec\r\n",
        "\r\n",
        "Global step: 23300 - [=================>--------]  74% - acc: 0.9609 - loss: 1.4997 - 7065.4 sample/sec\r\n",
        "\r\n",
        "Global step: 23310 - [=================>--------]  78% - acc: 0.8750 - loss: 1.5842 - 7112.8 sample/sec\r\n",
        "\r\n",
        "Global step: 23320 - [==================>--------]  81% - acc: 0.9141 - loss: 1.5463 - 7157.2 sample/sec\r\n",
        "\r\n",
        "Global step: 23330 - [===================>----------]  83% - acc: 0.9062 - loss: 1.5549 - 7159.3 sample/sec\r\n",
        "\r\n",
        "Global step: 23340 - [====================>-------]  85% - acc: 0.9219 - loss: 1.5389 - 7124.4 sample/sec\r\n",
        "\r\n",
        "Global step: 23350 - [====================>--------]  87% - acc: 0.9609 - loss: 1.5002 - 7175.4 sample/sec\r\n",
        "\r\n",
        "Global step: 23360 - [=====================>-------]  89% - acc: 0.9766 - loss: 1.4842 - 7194.2 sample/sec\r\n",
        "\r\n",
        "Global step: 23370 - [======================>-----]  90% - acc: 0.9375 - loss: 1.5231 - 7178.5 sample/sec\r\n",
        "\r\n",
        "Global step: 23380 - [======================>-----]  91% - acc: 0.8906 - loss: 1.5695 - 7173.2 sample/sec\r\n",
        "\r\n",
        "Global step: 23390 - [=======================>------]  92% - acc: 0.9375 - loss: 1.5225 - 7139.1 sample/sec\r\n",
        "\r\n",
        "Global step: 23400 - [========================>-----]  93% - acc: 0.9844 - loss: 1.4768 - 7140.1 sample/sec\r\n",
        "\r\n",
        "Global step: 23410 - [=========================>----]  94% - acc: 0.9766 - loss: 1.4840 - 7177.0 sample/sec\r\n",
        "\r\n",
        "Global step: 23420 - [==========================>---]  95% - acc: 0.9062 - loss: 1.5542 - 7192.1 sample/sec\r\n",
        "\r\n",
        "Global step: 23430 - [==========================>---]  97% - acc: 0.9297 - loss: 1.5313 - 7105.3 sample/sec\r\n",
        "\r\n",
        "Global step: 23440 - [===========================>--]  98% - acc: 0.9297 - loss: 1.5301 - 7153.3 sample/sec\r\n",
        "\r\n",
        "Global step: 23450 - [============================>-]  99% - acc: 0.9375 - loss: 1.5231 - 7175.7 sample/sec\r\n",
        "\r\n",
        "Global step: 23460 - [=============================>] 100% - acc: 0.9250 - loss: 1.5362 - 10247.5 sample/sec\r\n",
        "\r\n",
        "\r\n",
        "Epoch 60 - accuracy: 69.52% (6952/10000)\r\n",
        "\r\n",
        "Session is saved\r\n",
        "\r\n"
      ]
    }
  ]
}